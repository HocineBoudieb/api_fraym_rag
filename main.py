import os
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# Import du gestionnaire de sessions
from session_manager import SessionManager

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement
load_dotenv()

app = FastAPI(
    title="Fraym RAG API avec LangChain",
    description="API de recherche et g√©n√©ration augment√©e par r√©cup√©ration utilisant LangChain et ChromaDB",
    version="1.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic
class QueryRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    max_results: int = 5
    temperature: float = 0.7

class QueryResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    session_id: Optional[str] = None

class DocumentInfo(BaseModel):
    filename: str
    content_preview: str
    chunk_count: int
    metadata: Dict[str, Any]

# Mod√®les pour les sessions
class SessionCreate(BaseModel):
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class SessionResponse(BaseModel):
    id: str
    title: str
    created_at: str
    updated_at: str
    message_count: int
    metadata: Dict[str, Any]

class SessionUpdate(BaseModel):
    title: str

class MessageResponse(BaseModel):
    id: int
    timestamp: str
    role: str
    content: str
    metadata: Dict[str, Any]

# Configuration globale
class RAGSystem:
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None
        self.text_splitter = None
        self.knowledge_base_path = Path("knowledges")
        self.chroma_db_path = "./chroma_langchain_db"
        
        # Initialiser le gestionnaire de sessions
        self.session_manager = SessionManager("sessions.db")
        
        # Initialiser les composants
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialise les composants LangChain"""
        try:
            # V√©rifier la cl√© API OpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
            
            # Initialiser les embeddings
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=api_key,
                model="text-embedding-3-small"
            )
            
            # Initialiser le LLM
            self.llm = ChatOpenAI(
                openai_api_key=api_key,
                model="gpt-4o-mini",
                temperature=0.7
            )
            
            # Initialiser le text splitter
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            
            # Charger ou cr√©er la base vectorielle
            self._load_or_create_vectorstore()
            
            # Cr√©er la cha√Æne QA
            self._create_qa_chain()
            
            logger.info("‚úÖ Syst√®me RAG initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
    
    def _load_or_create_vectorstore(self):
        """Charge ou cr√©e la base vectorielle ChromaDB"""
        try:
            # Essayer de charger une base existante
            if os.path.exists(self.chroma_db_path):
                self.vectorstore = Chroma(
                    persist_directory=self.chroma_db_path,
                    embedding_function=self.embeddings
                )
                logger.info(f"üìö Base vectorielle charg√©e depuis {self.chroma_db_path}")
            else:
                # Cr√©er une nouvelle base
                self.vectorstore = Chroma(
                    persist_directory=self.chroma_db_path,
                    embedding_function=self.embeddings
                )
                logger.info(f"üÜï Nouvelle base vectorielle cr√©√©e dans {self.chroma_db_path}")
                
                # Charger les documents si le dossier knowledges existe
                if self.knowledge_base_path.exists():
                    self.load_knowledge_base()
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de la base vectorielle: {e}")
            raise
    
    def _create_qa_chain(self):
        """Cr√©e la cha√Æne de question-r√©ponse"""
        # Template de prompt personnalis√© pour g√©n√©rer du JSON selon le guide
        prompt_template = """
Tu es un assistant e-commerce sp√©cialis√© qui g√©n√®re des interfaces utilisateur dynamiques. Tu dois TOUJOURS r√©pondre avec un JSON valide selon la structure d√©finie dans le Guide de Structure JSON pour le Syst√®me de Rendu de Composants.

INSTRUCTIONS OBLIGATOIRES :
- Ta r√©ponse DOIT √™tre un JSON valide avec la structure : {{"template": "...", "components": [...], "templateProps": {{...}}}}
- Utilise les templates disponibles : "base", "centered", "grid", "dashboard", "landing"
- Utilise les composants appropri√©s : "Heading", "Text", "Button", "Card", "Grid", "ProductCard", "Container", "Navigation", etc.
- Pour les listes de produits : utilise "Grid" avec des "ProductCard"
- Pour les pages simples : utilise "centered" avec "Heading" et "Text"
- Pour les tableaux de bord : utilise "dashboard"
- Applique les classes Tailwind CSS appropri√©es
- Assure-toi que le JSON est syntaxiquement correct

STRUCTURE COH√âRENTE DES COMPOSANTS :
- TOUJOURS g√©n√©rer EXACTEMENT 6 composants dans cet ordre :
  1. Heading (titre principal ou message de bienvenue)
  2. ZaraCategoryButtons (boutons de cat√©gories)
  3. Text (texte descriptif ou question)
  4. Grid avec ProductCard OU ZaraProductGrid (produits)
- Maintenir cette structure pour TOUTES les r√©ponses
- Adapter uniquement le contenu, pas la structure

R√àGLES STRICTES POUR LES PRODUITS :
- Tu NE DOIS JAMAIS inventer ou cr√©er de nouveaux produits
- Tu DOIS UNIQUEMENT utiliser les produits, prix et informations pr√©sents dans le contexte fourni
- Si un produit n'existe pas dans le contexte, tu DOIS dire qu'il n'est pas disponible
- Tu NE DOIS PAS inventer de prix, de caract√©ristiques ou de descriptions
- Reste fid√®le aux informations exactes du catalogue fourni

Contexte:
{context}

Question: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure, en utilisant SEULEMENT les produits du contexte :"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Cr√©er la cha√Æne QA avec retriever am√©lior√©
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": 10  # R√©cup√©rer plus de documents pour avoir plus d'informations
                }
            ),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    def load_knowledge_base(self):
        """Charge tous les documents du dossier knowledges"""
        try:
            if not self.knowledge_base_path.exists():
                logger.warning(f"üìÅ Dossier {self.knowledge_base_path} non trouv√©")
                return
            
            # Charger tous les fichiers markdown, texte et JSON
            loader = DirectoryLoader(
                str(self.knowledge_base_path),
                glob="**/*.{md,txt,json}",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            
            documents = loader.load()
            logger.info(f"üìÑ {len(documents)} documents charg√©s")
            
            if documents:
                # Diviser les documents en chunks
                texts = self.text_splitter.split_documents(documents)
                
                # Enrichir les m√©tadonn√©es avec des tags
                for text in texts:
                    source_file = os.path.basename(text.metadata.get('source', ''))
                    content = text.page_content.lower()
                    
                    # Ajouter des tags bas√©s sur le contenu et le fichier source
                    tags = []
                    
                    # Tags bas√©s sur le nom du fichier
                    if 'product' in source_file or 'catalog' in source_file:
                        tags.extend(['product', 'catalog'])
                    if 'ecommerce' in source_file:
                        tags.extend(['ecommerce', 'general'])
                    if 'faq' in source_file:
                        tags.extend(['faq', 'support'])
                    if 'customer' in source_file:
                        tags.extend(['customer_service', 'support'])
                    
                    # Tags bas√©s sur le contenu
                    if any(word in content for word in ['prix', 'price', '‚Ç¨', 'euro']):
                        tags.append('pricing')
                    if any(word in content for word in ['iphone', 'samsung', 'macbook', 'dell', 'airpods']):
                        tags.extend(['product', 'electronics'])
                    if any(word in content for word in ['livraison', 'delivery', 'shipping']):
                        tags.append('shipping')
                    if any(word in content for word in ['garantie', 'warranty', 'sav']):
                        tags.append('warranty')
                    if any(word in content for word in ['paiement', 'payment', 'carte']):
                        tags.append('payment')
                    
                    # Ajouter les tags aux m√©tadonn√©es (convertir en string pour ChromaDB)
                    unique_tags = list(set(tags))  # Supprimer les doublons
                    text.metadata['tags'] = ','.join(unique_tags) if unique_tags else 'general'
                    text.metadata['content_type'] = 'product' if 'product' in tags else 'general'
                
                logger.info(f"‚úÇÔ∏è {len(texts)} chunks cr√©√©s avec m√©tadonn√©es enrichies")
                
                # Ajouter √† la base vectorielle
                self.vectorstore.add_documents(texts)
                self.vectorstore.persist()
                
                logger.info(f"‚úÖ Base de connaissances charg√©e: {len(texts)} chunks index√©s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de la base de connaissances: {e}")
            raise
    
    def detect_scenario(self, query: str, found_docs: List[Document]) -> str:
        """
        D√©tecte le sc√©nario appropri√© bas√© sur la requ√™te et les documents trouv√©s
        """
        query_lower = query.lower()
        
        # Mots-cl√©s par sc√©nario
        ecommerce_keywords = ['produit', 'acheter', 'prix', 'catalogue', 'recommandation', 'commander', 'panier']
        single_product_keywords = ['d√©tails', 'sp√©cifications', 'caract√©ristiques', 'fiche produit', 'plus d\'infos', 'description compl√®te']
        restaurant_keywords = ['menu', 'plat', 'restaurant', 'r√©server', 'carte', 'table', 'cuisine']
        support_keywords = ['aide', 'probl√®me', 'retour', 'livraison', 'garantie', 'support', 'contact']
        comparison_keywords = ['comparer', 'diff√©rence', 'vs', 'versus', 'mieux', 'choisir']
        landing_keywords = ['bonjour', 'salut', 'hello', 'bienvenue', 'aide-moi', 'que faire']
        
        # V√©rifier les tags dans les documents
        doc_tags = set()
        for doc in found_docs:
            if 'tags' in doc.metadata:
                if isinstance(doc.metadata['tags'], list):
                    doc_tags.update(doc.metadata['tags'])
                else:
                    doc_tags.add(doc.metadata['tags'])
        
        # Logique de d√©tection par priorit√©
        
        # 1. Produit unique avec d√©tails
        if ('product' in doc_tags or 'ecommerce' in doc_tags) and \
           (any(keyword in query_lower for keyword in single_product_keywords) or \
            len(found_docs) == 1):
            return 'single_product'
        
        # 2. Produits E-commerce (liste)
        if ('product' in doc_tags or 'ecommerce' in doc_tags) and \
           any(keyword in query_lower for keyword in ecommerce_keywords):
            return 'ecommerce_products'
        
        # 3. Restaurant
        if ('restaurant' in doc_tags or 'menu' in doc_tags) or \
           any(keyword in query_lower for keyword in restaurant_keywords):
            return 'restaurant_menu'
        
        # 4. Support/FAQ
        if ('support' in doc_tags or 'faq' in doc_tags) or \
           any(keyword in query_lower for keyword in support_keywords):
            return 'customer_support'
        
        # 5. Comparaison (si plusieurs produits et mots-cl√©s de comparaison)
        if any(keyword in query_lower for keyword in comparison_keywords) and \
           len(found_docs) > 1 and 'product' in doc_tags:
            return 'product_comparison'
        
        # 6. Landing/Accueil
        if any(keyword in query_lower for keyword in landing_keywords) or \
           len(query.strip()) < 20:  # Requ√™tes courtes = orientation
            return 'landing_page'
        
        # 7. Par d√©faut : informatif
        return 'informative'
    
    def get_scenario_prompt(self, scenario: str, session_context: str, context: str, question: str) -> str:
        """
        Retourne le prompt adapt√© au sc√©nario d√©tect√©
        """
        base_instructions = """
Tu es un assistant e-commerce sp√©cialis√© qui g√©n√®re des interfaces utilisateur dynamiques. Tu dois TOUJOURS r√©pondre avec un JSON valide selon la structure d√©finie dans le Guide de Structure JSON pour le Syst√®me de Rendu de Composants.

INSTRUCTIONS OBLIGATOIRES :
- Ta r√©ponse DOIT √™tre un JSON valide avec la structure : {{"template": "...", "components": [...], "templateProps": {{...}}}}
- Utilise les templates disponibles : "base", "centered", "grid", "dashboard", "landing"
- Utilise les composants appropri√©s selon le sc√©nario
- Applique les classes Tailwind CSS appropri√©es
- Assure-toi que le JSON est syntaxiquement correct
- Prends en compte l'historique de la conversation pour maintenir la coh√©rence
"""
        
        scenario_prompts = {
            'single_product': f"""{base_instructions}

SC√âNARIO : PRODUIT UNIQUE - FICHE D√âTAILL√âE
- Utilise le template "centered" pour une pr√©sentation focalis√©e
- Structure recommand√©e : Heading ‚Üí ProductCard d√©taill√©e ‚Üí Text compl√©mentaires ‚Üí Button d'action
- Mets en avant TOUS les d√©tails du produit unique :
  * Nom, prix, description compl√®te
  * Sp√©cifications techniques d√©taill√©es
  * Caract√©ristiques importantes
  * Informations de disponibilit√©
- Ajoute des informations sur la livraison, garantie, service apr√®s-vente
- Inclus un appel √† l'action clair ("Ajouter au panier", "Commander")
- Optimise pour la conversion sur ce produit sp√©cifique

R√àGLES STRICTES POUR LES PRODUITS :
- Tu NE DOIS JAMAIS inventer ou cr√©er de nouveaux produits
- Tu DOIS UNIQUEMENT utiliser les produits, prix et informations pr√©sents dans le contexte fourni
- Si un produit n'existe pas dans le contexte, tu DOIS dire qu'il n'est pas disponible

Historique de la conversation:
{session_context}

Contexte du produit:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :""",
            
            'ecommerce_products': f"""{base_instructions}

SC√âNARIO : AFFICHAGE DE PRODUITS E-COMMERCE
- Utilise le template "grid" pour une pr√©sentation optimale des produits
- Structure recommand√©e : Heading ‚Üí Text descriptif ‚Üí Grid avec ProductCard ‚Üí Text de suivi/CTA
- Mets en avant les produits avec leurs caract√©ristiques et prix
- Inclus des appels √† l'action pour l'achat
- Optimise pour la conversion

R√àGLES STRICTES POUR LES PRODUITS :
- Tu NE DOIS JAMAIS inventer ou cr√©er de nouveaux produits
- Tu DOIS UNIQUEMENT utiliser les produits, prix et informations pr√©sents dans le contexte fourni
- Si un produit n'existe pas dans le contexte, tu DOIS dire qu'il n'est pas disponible

Historique de la conversation:
{session_context}

Contexte des produits:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :""",
            
            'restaurant_menu': f"""{base_instructions}

SC√âNARIO : MENU RESTAURANT
- Utilise le template "centered" ou "grid" selon le contenu
- Structure recommand√©e : Heading ‚Üí Grid avec Cards pour les plats ‚Üí Button de r√©servation
- Mets en avant les sp√©cialit√©s et informations pratiques
- Inclus les informations de contact et r√©servation

Historique de la conversation:
{session_context}

Contexte restaurant:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :""",
            
            'customer_support': f"""{base_instructions}

SC√âNARIO : SERVICE CLIENT / SUPPORT
- Utilise le template "centered" pour une lecture facile
- Structure recommand√©e : Heading ‚Üí Card avec r√©ponse d√©taill√©e ‚Üí Text + Button de contact
- Priorise la clart√© et l'aide pratique
- Inclus les informations de contact si pertinent

Historique de la conversation:
{session_context}

Contexte support:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :""",
            
            'product_comparison': f"""{base_instructions}

SC√âNARIO : COMPARAISON DE PRODUITS
- Utilise le template "grid" pour comparer c√¥te √† c√¥te
- Structure recommand√©e : Heading ‚Üí Grid avec Cards de comparaison ‚Üí Text de recommandation
- Mets en √©vidence les diff√©rences cl√©s
- Conclus avec une recommandation bas√©e sur les besoins

Historique de la conversation:
{session_context}

Contexte des produits √† comparer:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :""",
            
            'landing_page': f"""{base_instructions}

SC√âNARIO : PAGE D'ACCUEIL / ORIENTATION
- Utilise le template "landing" pour un accueil chaleureux
- Structure recommand√©e : Heading de bienvenue ‚Üí Text d'orientation ‚Üí Grid avec options d'action
- Propose des directions claires vers les principales fonctionnalit√©s
- Cr√©e une exp√©rience d'onboarding fluide

Historique de la conversation:
{session_context}

Contexte g√©n√©ral:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :""",
            
            'informative': f"""{base_instructions}

SC√âNARIO : R√âPONSE INFORMATIVE
- Utilise le template "centered" pour une pr√©sentation claire
- Structure recommand√©e : Heading ‚Üí Text/Card avec information ‚Üí Text de suivi si n√©cessaire
- Priorise la clart√© et la pertinence de l'information
- Garde une structure simple et lisible

Historique de la conversation:
{session_context}

Contexte:
{context}

Question actuelle: {question}

R√©ponds UNIQUEMENT avec un JSON valide selon le guide de structure :"""
        }
        
        return scenario_prompts.get(scenario, scenario_prompts['informative'])
    
    def get_chunks_by_tag(self, tag: str, limit: int = 10) -> List[Document]:
        """R√©cup√®re les chunks ayant un tag sp√©cifique"""
        try:
            collection = self.vectorstore._collection
            # R√©cup√©rer tous les documents et filtrer manuellement
            results = collection.get(
                include=["documents", "metadatas"]
            )
            
            filtered_docs = []
            for i, metadata in enumerate(results.get('metadatas', [])):
                if metadata and 'tags' in metadata:
                    tags_str = metadata['tags']
                    if tag in tags_str.split(','):
                        doc = Document(
                            page_content=results['documents'][i],
                            metadata=metadata
                        )
                        filtered_docs.append(doc)
                        if len(filtered_docs) >= limit:
                            break
            
            return filtered_docs
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration par tag: {e}")
            return []
    
    def query(self, question: str, session_id: str = None, max_results: int = 5) -> Dict[str, Any]:
        """Effectue une requ√™te sur la base de connaissances avec logique am√©lior√©e et gestion de session"""
        try:
            if not self.qa_chain:
                raise ValueError("Syst√®me QA non initialis√©")
            
            # Cr√©er une session si n√©cessaire
            if not session_id:
                session_id = self.session_manager.create_session()
                logger.info(f"üÜï Nouvelle session cr√©√©e: {session_id}")
            
            # Enregistrer la question de l'utilisateur
            self.session_manager.add_message(session_id, "user", question)
            
            # R√©cup√©rer le contexte de la session
            session_context = self.session_manager.get_session_context(session_id, max_messages=5)
            
            question_lower = question.lower()
            
            # D√©tecter si la question concerne les produits (logique √©largie)
            product_keywords = [
                # Mots directs
                'produit', 'product', 'liste', 'catalog', 'catalogue', 'disponible', 'prix',
                # Produits sp√©cifiques
                'smartphone', 'ordinateur', 'iphone', 'samsung', 'macbook', 'airpods', 'dell',
                # Actions d'achat/recommandation
                'acheter', 'achat', 'buy', 'purchase', 'commander', 'order',
                'cadeau', 'cadeaux', 'gift', 'offrir', 'offer',
                'proposer', 'propose', 'recommander', 'recommend', 'sugg√©rer', 'suggest',
                'cherche', 'search', 'trouve', 'find', 'besoin', 'need', 'veux', 'want',
                # Contextes commerciaux
                'boutique', 'magasin', 'shop', 'store', 'vendre', 'sell', 'vente', 'sale',
                'choisir', 'choose', 's√©lectionner', 'select', 'comparer', 'compare',
                # Termes g√©n√©raux qui impliquent souvent des produits
                'que me', 'qu\'avez', 'avez-vous', 'do you have', 'what do you',
                'me conseillez', 'me proposez', 'me recommandez'
            ]
            is_product_query = any(keyword in question_lower for keyword in product_keywords)
            
            if is_product_query:
                # Pour les questions sur les produits, r√©cup√©rer tous les chunks avec tag 'product'
                logger.info("üè∑Ô∏è Requ√™te produit d√©tect√©e - r√©cup√©ration par tag")
                product_docs = self.get_chunks_by_tag('product', limit=15)
                
                if product_docs:
                    # D√©tecter le sc√©nario appropri√©
                    scenario = self.detect_scenario(question, product_docs)
                    logger.info(f"üìã Sc√©nario d√©tect√©: {scenario}")
                    
                    # Cr√©er un contexte sp√©cialis√© pour les produits
                    context = "\n\n".join([doc.page_content for doc in product_docs])
                    
                    # Obtenir le prompt adapt√© au sc√©nario
                    formatted_prompt = self.get_scenario_prompt(scenario, session_context, context, question)
                    
                    # G√©n√©rer la r√©ponse
                    response = self.llm.invoke(formatted_prompt)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    
                    # Enregistrer la r√©ponse de l'assistant
                    self.session_manager.add_message(session_id, "assistant", answer, {
                        "search_method": "tag_based",
                        "tag_used": "product",
                        "scenario": scenario,
                        "sources_count": len(product_docs)
                    })
                    
                    # Formater les sources
                    sources = []
                    for doc in product_docs:
                        sources.append({
                            "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                            "metadata": doc.metadata,
                            "source": doc.metadata.get("source", "Unknown")
                        })
                    
                    return {
                        "answer": answer,
                        "sources": sources,
                        "session_id": session_id,
                        "metadata": {
                            "total_sources": len(product_docs),
                            "query": question,
                            "search_method": "tag_based",
                            "scenario": scenario,
                            "tag_used": "product"
                        }
                    }
            
            # Pour les autres questions, utiliser la recherche par similarit√© normale
            logger.info("üîç Requ√™te g√©n√©rale - recherche par similarit√©")
            
            # Modifier le prompt pour inclure l'historique de session
            if session_context:
                # Cr√©er un prompt enrichi avec l'historique
                enriched_query = f"Historique de la conversation:\n{session_context}\n\nQuestion actuelle: {question}"
                result = self.qa_chain({"query": enriched_query})
            else:
                result = self.qa_chain({"query": question})
            
            answer = result["result"]
            sources_found = result.get("source_documents", [])
            
            # D√©tecter le sc√©nario pour les r√©ponses g√©n√©rales
            scenario = self.detect_scenario(question, sources_found)
            logger.info(f"üìã Sc√©nario g√©n√©ral d√©tect√©: {scenario}")
            
            # Si le sc√©nario d√©tect√© n√©cessite un format JSON sp√©cifique, r√©g√©n√©rer la r√©ponse
            if scenario in ['restaurant_menu', 'customer_support', 'landing_page', 'product_comparison']:
                context = "\n\n".join([doc.page_content for doc in sources_found]) if sources_found else "Aucun contexte sp√©cifique trouv√©."
                formatted_prompt = self.get_scenario_prompt(scenario, session_context, context, question)
                
                # R√©g√©n√©rer avec le format adapt√©
                response = self.llm.invoke(formatted_prompt)
                answer = response.content if hasattr(response, 'content') else str(response)
            
            # Logique de fallback : si peu de sources trouv√©es et que la question pourrait concerner des recommandations
            fallback_keywords = ['recommand', 'conseil', 'suggest', 'propose', 'que faire', 'quoi', 'help', 'aide']
            should_try_products = (
                len(sources_found) < 3 and 
                any(keyword in question_lower for keyword in fallback_keywords)
            )
            
            if should_try_products:
                logger.info("üîÑ Fallback - tentative de recherche dans les produits")
                product_docs = self.get_chunks_by_tag('product', limit=10)
                
                if product_docs and len(product_docs) > len(sources_found):
                    # D√©tecter le sc√©nario appropri√© pour le fallback
                    scenario = self.detect_scenario(question, product_docs)
                    logger.info(f"üìã Sc√©nario fallback d√©tect√©: {scenario}")
                    
                    # Utiliser les produits comme sources suppl√©mentaires
                    context = "\n\n".join([doc.page_content for doc in product_docs])
                    
                    # Obtenir le prompt adapt√© au sc√©nario
                    formatted_prompt = self.get_scenario_prompt(scenario, session_context, context, question)
                    
                    # G√©n√©rer la r√©ponse avec les produits
                    response = self.llm.invoke(formatted_prompt)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    
                    # Enregistrer la r√©ponse de l'assistant
                    self.session_manager.add_message(session_id, "assistant", answer, {
                        "search_method": "fallback_products",
                        "scenario": scenario,
                        "sources_count": len(product_docs)
                    })
                    
                    # Formater les sources avec les produits
                    sources = []
                    for doc in product_docs:
                        sources.append({
                            "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                            "metadata": doc.metadata,
                            "source": doc.metadata.get("source", "Unknown")
                        })
                    
                    return {
                        "answer": answer,
                        "sources": sources,
                        "session_id": session_id,
                        "metadata": {
                            "total_sources": len(product_docs),
                            "query": question,
                            "search_method": "fallback_products",
                            "scenario": scenario
                        }
                    }
            
            # Enregistrer la r√©ponse de l'assistant (recherche normale)
            self.session_manager.add_message(session_id, "assistant", answer, {
                "search_method": "similarity",
                "scenario": scenario,
                "sources_count": len(sources_found)
            })
            
            # Formater les sources
            sources = []
            for doc in sources_found:
                sources.append({
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "metadata": doc.metadata,
                    "source": doc.metadata.get("source", "Unknown")
                })
            
            return {
                "answer": answer,
                "sources": sources[:max_results],
                "session_id": session_id,
                "metadata": {
                    "total_sources": len(sources_found),
                    "query": question,
                    "search_method": "similarity",
                    "scenario": scenario
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la requ√™te: {e}")
            raise
    
    def get_collection_info(self) -> Dict[str, Any]:
        """Retourne des informations sur la collection"""
        try:
            if not self.vectorstore:
                return {"status": "not_initialized", "count": 0}
            
            # Obtenir le nombre de documents
            collection = self.vectorstore._collection
            count = collection.count()
            
            return {
                "status": "ready",
                "count": count,
                "embedding_model": "text-embedding-3-small",
                "llm_model": "gpt-4o-mini"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des infos: {e}")
            return {"status": "error", "error": str(e)}

# Instance globale du syst√®me RAG
rag_system = RAGSystem()

# Routes API
@app.get("/")
async def root():
    return {
        "message": "Fraym RAG API avec LangChain",
        "version": "1.0.0",
        "status": "active"
    }

@app.get("/health")
async def health_check():
    info = rag_system.get_collection_info()
    return {
        "status": "healthy",
        "vectorstore": info
    }

@app.post("/query", response_model=QueryResponse)
async def query_knowledge_base(request: QueryRequest):
    """Effectue une requ√™te sur la base de connaissances avec gestion de session"""
    try:
        result = rag_system.query(
            question=request.query,
            session_id=request.session_id,
            max_results=request.max_results
        )
        
        return QueryResponse(
            answer=result["answer"],
            sources=result["sources"],
            metadata=result["metadata"],
            session_id=result["session_id"]
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la requ√™te: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reload")
async def reload_knowledge_base():
    """Recharge la base de connaissances"""
    try:
        rag_system.load_knowledge_base()
        info = rag_system.get_collection_info()
        
        return {
            "status": "success",
            "message": "Base de connaissances recharg√©e",
            "info": info
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du rechargement: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/info")
async def get_system_info():
    """Retourne des informations sur le syst√®me"""
    info = rag_system.get_collection_info()
    
    return {
        "system": "Fraym RAG avec LangChain",
        "vectorstore": info,
        "knowledge_path": str(rag_system.knowledge_base_path),
        "chroma_path": rag_system.chroma_db_path
    }

# Routes pour la gestion des sessions
@app.post("/sessions", response_model=SessionResponse)
async def create_session(request: SessionCreate):
    """Cr√©e une nouvelle session de conversation"""
    try:
        session_id = rag_system.session_manager.create_session(request.title)
        return SessionResponse(
            session_id=session_id,
            title=request.title,
            created_at=rag_system.session_manager.get_session_info(session_id)["created_at"],
            message_count=0
        )
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation de session: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sessions")
async def list_sessions():
    """Liste toutes les sessions"""
    try:
        sessions = rag_system.session_manager.get_sessions()
        return {"sessions": sessions}
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des sessions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sessions/{session_id}/history")
async def get_session_history(session_id: str):
    """R√©cup√®re l'historique d'une session"""
    try:
        if not rag_system.session_manager.session_exists(session_id):
            raise HTTPException(status_code=404, detail="Session non trouv√©e")
        
        history = rag_system.session_manager.get_session_history(session_id)
        messages = []
        for msg in history:
            messages.append(MessageResponse(
                role=msg["role"],
                content=msg["content"],
                timestamp=msg["timestamp"],
                metadata=msg["metadata"]
            ))
        
        return {"session_id": session_id, "messages": messages}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration de l'historique: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/sessions/{session_id}")
async def update_session(session_id: str, request: SessionUpdate):
    """Met √† jour le titre d'une session"""
    try:
        if not rag_system.session_manager.session_exists(session_id):
            raise HTTPException(status_code=404, detail="Session non trouv√©e")
        
        rag_system.session_manager.update_session_title(session_id, request.title)
        return {"message": "Session mise √† jour avec succ√®s"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la mise √† jour de session: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """Supprime une session"""
    try:
        if not rag_system.session_manager.session_exists(session_id):
            raise HTTPException(status_code=404, detail="Session non trouv√©e")
        
        rag_system.session_manager.delete_session(session_id)
        return {"message": "Session supprim√©e avec succ√®s"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la suppression de session: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)