import os
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement
load_dotenv()

app = FastAPI(
    title="Fraym RAG API avec LangChain",
    description="API de recherche et g√©n√©ration augment√©e par r√©cup√©ration utilisant LangChain et ChromaDB",
    version="1.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic
class QueryRequest(BaseModel):
    query: str
    max_results: int = 5
    temperature: float = 0.7

class QueryResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]

class DocumentInfo(BaseModel):
    filename: str
    content_preview: str
    chunk_count: int
    metadata: Dict[str, Any]

# Configuration globale
class RAGSystem:
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None
        self.text_splitter = None
        self.knowledge_base_path = Path("knowledges")
        self.chroma_db_path = "./chroma_langchain_db"
        
        # Initialiser les composants
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialise les composants LangChain"""
        try:
            # V√©rifier la cl√© API OpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
            
            # Initialiser les embeddings
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=api_key,
                model="text-embedding-3-small"
            )
            
            # Initialiser le LLM
            self.llm = ChatOpenAI(
                openai_api_key=api_key,
                model="gpt-4o-mini",
                temperature=0.7
            )
            
            # Initialiser le text splitter
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            
            # Charger ou cr√©er la base vectorielle
            self._load_or_create_vectorstore()
            
            # Cr√©er la cha√Æne QA
            self._create_qa_chain()
            
            logger.info("‚úÖ Syst√®me RAG initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
    
    def _load_or_create_vectorstore(self):
        """Charge ou cr√©e la base vectorielle ChromaDB"""
        try:
            # Essayer de charger une base existante
            if os.path.exists(self.chroma_db_path):
                self.vectorstore = Chroma(
                    persist_directory=self.chroma_db_path,
                    embedding_function=self.embeddings
                )
                logger.info(f"üìö Base vectorielle charg√©e depuis {self.chroma_db_path}")
            else:
                # Cr√©er une nouvelle base
                self.vectorstore = Chroma(
                    persist_directory=self.chroma_db_path,
                    embedding_function=self.embeddings
                )
                logger.info(f"üÜï Nouvelle base vectorielle cr√©√©e dans {self.chroma_db_path}")
                
                # Charger les documents si le dossier knowledges existe
                if self.knowledge_base_path.exists():
                    self.load_knowledge_base()
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de la base vectorielle: {e}")
            raise
    
    def _create_qa_chain(self):
        """Cr√©e la cha√Æne de question-r√©ponse"""
        # Template de prompt personnalis√©
        prompt_template = """
Tu es un assistant e-commerce sp√©cialis√©. Utilise le contexte suivant pour r√©pondre √† la question de mani√®re pr√©cise et structur√©e.

INSTRUCTIONS SP√âCIALES :
- Pour les demandes de liste de produits : pr√©sente TOUS les produits disponibles avec leurs caract√©ristiques principales (nom, prix, sp√©cifications cl√©s)
- Pour les filtres (ex: "produits pas chers", "moins de X‚Ç¨") : analyse les prix et filtre automatiquement
- Pour les comparaisons : pr√©sente les diff√©rences cl√©s entre les produits
- Utilise des listes √† puces et une mise en forme claire
- Inclus toujours les prix quand disponibles
- Si des informations manquent, indique-le clairement

Contexte:
{context}

Question: {question}

R√©ponse structur√©e et d√©taill√©e:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Cr√©er la cha√Æne QA avec retriever am√©lior√©
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": 10  # R√©cup√©rer plus de documents pour avoir plus d'informations
                }
            ),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    def load_knowledge_base(self):
        """Charge tous les documents du dossier knowledges"""
        try:
            if not self.knowledge_base_path.exists():
                logger.warning(f"üìÅ Dossier {self.knowledge_base_path} non trouv√©")
                return
            
            # Charger tous les fichiers markdown, texte et JSON
            loader = DirectoryLoader(
                str(self.knowledge_base_path),
                glob="**/*.{md,txt,json}",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            
            documents = loader.load()
            logger.info(f"üìÑ {len(documents)} documents charg√©s")
            
            if documents:
                # Diviser les documents en chunks
                texts = self.text_splitter.split_documents(documents)
                
                # Enrichir les m√©tadonn√©es avec des tags
                for text in texts:
                    source_file = os.path.basename(text.metadata.get('source', ''))
                    content = text.page_content.lower()
                    
                    # Ajouter des tags bas√©s sur le contenu et le fichier source
                    tags = []
                    
                    # Tags bas√©s sur le nom du fichier
                    if 'product' in source_file or 'catalog' in source_file:
                        tags.extend(['product', 'catalog'])
                    if 'ecommerce' in source_file:
                        tags.extend(['ecommerce', 'general'])
                    if 'faq' in source_file:
                        tags.extend(['faq', 'support'])
                    if 'customer' in source_file:
                        tags.extend(['customer_service', 'support'])
                    
                    # Tags bas√©s sur le contenu
                    if any(word in content for word in ['prix', 'price', '‚Ç¨', 'euro']):
                        tags.append('pricing')
                    if any(word in content for word in ['iphone', 'samsung', 'macbook', 'dell', 'airpods']):
                        tags.extend(['product', 'electronics'])
                    if any(word in content for word in ['livraison', 'delivery', 'shipping']):
                        tags.append('shipping')
                    if any(word in content for word in ['garantie', 'warranty', 'sav']):
                        tags.append('warranty')
                    if any(word in content for word in ['paiement', 'payment', 'carte']):
                        tags.append('payment')
                    
                    # Ajouter les tags aux m√©tadonn√©es (convertir en string pour ChromaDB)
                    unique_tags = list(set(tags))  # Supprimer les doublons
                    text.metadata['tags'] = ','.join(unique_tags) if unique_tags else 'general'
                    text.metadata['content_type'] = 'product' if 'product' in tags else 'general'
                
                logger.info(f"‚úÇÔ∏è {len(texts)} chunks cr√©√©s avec m√©tadonn√©es enrichies")
                
                # Ajouter √† la base vectorielle
                self.vectorstore.add_documents(texts)
                self.vectorstore.persist()
                
                logger.info(f"‚úÖ Base de connaissances charg√©e: {len(texts)} chunks index√©s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de la base de connaissances: {e}")
            raise
    
    def get_chunks_by_tag(self, tag: str, limit: int = 10) -> List[Document]:
        """R√©cup√®re les chunks ayant un tag sp√©cifique"""
        try:
            collection = self.vectorstore._collection
            # R√©cup√©rer tous les documents et filtrer manuellement
            results = collection.get(
                include=["documents", "metadatas"]
            )
            
            filtered_docs = []
            for i, metadata in enumerate(results.get('metadatas', [])):
                if metadata and 'tags' in metadata:
                    tags_str = metadata['tags']
                    if tag in tags_str.split(','):
                        doc = Document(
                            page_content=results['documents'][i],
                            metadata=metadata
                        )
                        filtered_docs.append(doc)
                        if len(filtered_docs) >= limit:
                            break
            
            return filtered_docs
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration par tag: {e}")
            return []
    
    def query(self, question: str, max_results: int = 5) -> Dict[str, Any]:
        """Effectue une requ√™te sur la base de connaissances avec logique am√©lior√©e"""
        try:
            if not self.qa_chain:
                raise ValueError("Syst√®me QA non initialis√©")
            
            question_lower = question.lower()
            
            # D√©tecter si la question concerne les produits
            product_keywords = ['produit', 'product', 'liste', 'catalog', 'catalogue', 'disponible', 'prix', 'smartphone', 'ordinateur', 'iphone', 'samsung', 'macbook']
            is_product_query = any(keyword in question_lower for keyword in product_keywords)
            
            if is_product_query:
                # Pour les questions sur les produits, r√©cup√©rer tous les chunks avec tag 'product'
                logger.info("üè∑Ô∏è Requ√™te produit d√©tect√©e - r√©cup√©ration par tag")
                product_docs = self.get_chunks_by_tag('product', limit=15)
                
                if product_docs:
                    # Cr√©er un contexte sp√©cialis√© pour les produits
                    context = "\n\n".join([doc.page_content for doc in product_docs])
                    
                    # Utiliser le prompt avec le contexte enrichi
                    prompt_template = self.qa_chain.combine_documents_chain.llm_chain.prompt
                    formatted_prompt = prompt_template.format(
                        context=context,
                        question=question
                    )
                    
                    # G√©n√©rer la r√©ponse
                    response = self.llm.invoke(formatted_prompt)
                    
                    # Formater les sources
                    sources = []
                    for doc in product_docs:
                        sources.append({
                            "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                            "metadata": doc.metadata,
                            "source": doc.metadata.get("source", "Unknown")
                        })
                    
                    return {
                        "answer": response.content if hasattr(response, 'content') else str(response),
                        "sources": sources,
                        "metadata": {
                            "total_sources": len(product_docs),
                            "query": question,
                            "search_method": "tag_based",
                            "tag_used": "product"
                        }
                    }
            
            # Pour les autres questions, utiliser la recherche par similarit√© normale
            logger.info("üîç Requ√™te g√©n√©rale - recherche par similarit√©")
            result = self.qa_chain({"query": question})
            
            # Formater les sources
            sources = []
            for doc in result.get("source_documents", []):
                sources.append({
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "metadata": doc.metadata,
                    "source": doc.metadata.get("source", "Unknown")
                })
            
            return {
                "answer": result["result"],
                "sources": sources[:max_results],
                "metadata": {
                    "total_sources": len(result.get("source_documents", [])),
                    "query": question,
                    "search_method": "similarity"
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la requ√™te: {e}")
            raise
    
    def get_collection_info(self) -> Dict[str, Any]:
        """Retourne des informations sur la collection"""
        try:
            if not self.vectorstore:
                return {"status": "not_initialized", "count": 0}
            
            # Obtenir le nombre de documents
            collection = self.vectorstore._collection
            count = collection.count()
            
            return {
                "status": "ready",
                "count": count,
                "embedding_model": "text-embedding-3-small",
                "llm_model": "gpt-4o-mini"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des infos: {e}")
            return {"status": "error", "error": str(e)}

# Instance globale du syst√®me RAG
rag_system = RAGSystem()

# Routes API
@app.get("/")
async def root():
    return {
        "message": "Fraym RAG API avec LangChain",
        "version": "1.0.0",
        "status": "active"
    }

@app.get("/health")
async def health_check():
    info = rag_system.get_collection_info()
    return {
        "status": "healthy",
        "vectorstore": info
    }

@app.post("/query", response_model=QueryResponse)
async def query_knowledge_base(request: QueryRequest):
    """Effectue une requ√™te sur la base de connaissances"""
    try:
        result = rag_system.query(
            question=request.query,
            max_results=request.max_results
        )
        
        return QueryResponse(
            answer=result["answer"],
            sources=result["sources"],
            metadata=result["metadata"]
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la requ√™te: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reload")
async def reload_knowledge_base():
    """Recharge la base de connaissances"""
    try:
        rag_system.load_knowledge_base()
        info = rag_system.get_collection_info()
        
        return {
            "status": "success",
            "message": "Base de connaissances recharg√©e",
            "info": info
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du rechargement: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/info")
async def get_system_info():
    """Retourne des informations sur le syst√®me"""
    info = rag_system.get_collection_info()
    
    return {
        "system": "Fraym RAG avec LangChain",
        "vectorstore": info,
        "knowledge_path": str(rag_system.knowledge_base_path),
        "chroma_path": rag_system.chroma_db_path
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)