# Fraym RAG System avec LangChain

SystÃ¨me de Recherche et GÃ©nÃ©ration AugmentÃ©e par RÃ©cupÃ©ration (RAG) utilisant LangChain, ChromaDB et OpenAI.

## ğŸš€ FonctionnalitÃ©s

- **RAG avec LangChain** : Utilisation du framework LangChain pour une architecture modulaire
- **Embeddings OpenAI** : ModÃ¨le `text-embedding-3-small` pour des embeddings de haute qualitÃ©
- **LLM GPT-4o-mini** : GÃ©nÃ©ration de rÃ©ponses intelligentes et contextuelles
- **ChromaDB** : Base de donnÃ©es vectorielle performante
- **API FastAPI** : Interface REST moderne et documentÃ©e
- **Chunking intelligent** : Division automatique des documents avec RecursiveCharacterTextSplitter
- **Support multi-formats** : Markdown, texte et JSON

## ğŸ“‹ PrÃ©requis

- Python 3.9+
- ClÃ© API OpenAI
- uv (gestionnaire de paquets Python)

## ğŸ› ï¸ Installation

1. **Cloner le projet**
```bash
git clone <repository-url>
cd test_dev
```

2. **Installer les dÃ©pendances**
```bash
uv sync
```

3. **Configuration**
CrÃ©er un fichier `.env` avec votre clÃ© API OpenAI :
```bash
cp .env.example .env
# Ã‰diter .env et ajouter votre OPENAI_API_KEY
```

## ğŸš€ DÃ©marrage rapide

### 1. Initialiser la base de connaissances

```bash
uv run python init_knowledge_base.py
```

Ce script va :
- Charger tous les documents du dossier `knowledges/`
- Les diviser en chunks intelligents
- CrÃ©er les embeddings avec OpenAI
- Sauvegarder dans ChromaDB

### 2. DÃ©marrer l'API

```bash
uv run python main.py
```

L'API sera disponible sur `http://localhost:8000`

### 3. Tester le systÃ¨me

```bash
uv run python test_rag.py
```

## ğŸ“š Structure du projet

```
.
â”œâ”€â”€ main.py                 # API FastAPI principale
â”œâ”€â”€ init_knowledge_base.py  # Script d'initialisation
â”œâ”€â”€ test_rag.py            # Script de test
â”œâ”€â”€ knowledges/            # Documents Ã  indexer
â”‚   â”œâ”€â”€ product_catalog.md
â”‚   â”œâ”€â”€ faq.md
â”‚   â”œâ”€â”€ customer_service.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chroma_langchain_db/   # Base ChromaDB (crÃ©Ã©e automatiquement)
â”œâ”€â”€ .env                   # Variables d'environnement
â””â”€â”€ pyproject.toml         # DÃ©pendances
```

## ğŸ”§ Configuration

### Variables d'environnement (.env)

```bash
OPENAI_API_KEY=your_openai_api_key_here
```

### ParamÃ¨tres du systÃ¨me

- **ModÃ¨le embedding** : `text-embedding-3-small`
- **ModÃ¨le LLM** : `gpt-4o-mini`
- **Taille des chunks** : 1000 caractÃ¨res
- **Overlap** : 200 caractÃ¨res
- **RÃ©sultats par dÃ©faut** : 5 documents

## ğŸ“– Utilisation de l'API

### Endpoints principaux

#### GET `/`
Informations sur l'API

#### GET `/health`
VÃ©rification de l'Ã©tat du systÃ¨me

#### POST `/query`
Effectuer une requÃªte sur la base de connaissances

```json
{
  "query": "Quels produits sont disponibles ?",
  "max_results": 5,
  "temperature": 0.7
}
```

RÃ©ponse :
```json
{
  "answer": "Voici les produits disponibles...",
  "sources": [
    {
      "content": "Extrait du document...",
      "metadata": {"source": "product_catalog.md"},
      "source": "product_catalog.md"
    }
  ],
  "metadata": {
    "total_sources": 3,
    "query": "Quels produits sont disponibles ?"
  }
}
```

#### POST `/reload`
Recharger la base de connaissances

#### GET `/info`
Informations dÃ©taillÃ©es sur le systÃ¨me

### Documentation interactive

AccÃ©dez Ã  `http://localhost:8000/docs` pour la documentation Swagger interactive.

## ğŸ§ª Tests

### Test automatique
```bash
uv run python test_rag.py
```

### Test manuel avec curl
```bash
# Test de santÃ©
curl http://localhost:8000/health

# RequÃªte simple
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"query": "Quels sont les produits disponibles ?"}'
```

## ğŸ” Architecture LangChain

### Composants utilisÃ©s

1. **Document Loaders** : `DirectoryLoader` + `TextLoader`
2. **Text Splitter** : `RecursiveCharacterTextSplitter`
3. **Embeddings** : `OpenAIEmbeddings`
4. **Vector Store** : `Chroma`
5. **LLM** : `ChatOpenAI`
6. **Chain** : `RetrievalQA`

### Flux de traitement

1. **Chargement** : Les documents sont chargÃ©s depuis `knowledges/`
2. **Chunking** : Division en chunks avec overlap
3. **Embedding** : Conversion en vecteurs avec OpenAI
4. **Stockage** : Sauvegarde dans ChromaDB
5. **RequÃªte** : Recherche par similaritÃ© + gÃ©nÃ©ration de rÃ©ponse
6. **RÃ©ponse** : RÃ©ponse contextuelle avec sources

## ğŸ“Š Monitoring

### Logs
Les logs sont affichÃ©s dans la console avec diffÃ©rents niveaux :
- `INFO` : OpÃ©rations normales
- `WARNING` : Avertissements
- `ERROR` : Erreurs

### MÃ©triques
- Nombre de documents indexÃ©s
- Temps de rÃ©ponse des requÃªtes
- Sources utilisÃ©es par requÃªte

## ğŸ”§ Personnalisation

### Modifier le prompt
Ã‰ditez le `prompt_template` dans `main.py` :

```python
prompt_template = """
Votre prompt personnalisÃ© ici...

Contexte:
{context}

Question: {question}

RÃ©ponse:"""
```

### Changer les paramÃ¨tres de chunking
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Taille des chunks
    chunk_overlap=300,  # Overlap
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

### Modifier la recherche
```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 10}  # Nombre de documents
)
```

## ğŸ› DÃ©pannage

### Erreurs communes

1. **ClÃ© API manquante**
   ```
   ValueError: OPENAI_API_KEY non trouvÃ©e
   ```
   â†’ VÃ©rifiez votre fichier `.env`

2. **Dossier knowledges vide**
   ```
   Aucun document trouvÃ©
   ```
   â†’ Ajoutez des fichiers `.md` ou `.txt` dans `knowledges/`

3. **Erreur ChromaDB**
   ```
   Erreur lors du chargement de la base vectorielle
   ```
   â†’ Supprimez le dossier `chroma_langchain_db/` et rÃ©initialisez

### Reset complet
```bash
# Supprimer la base
rm -rf chroma_langchain_db/

# RÃ©initialiser
uv run python init_knowledge_base.py
```

## ğŸ“ˆ Performance

### Optimisations
- Utilisation de `text-embedding-3-small` (plus rapide que `text-embedding-ada-002`)
- Chunking intelligent avec overlap
- Cache des embeddings dans ChromaDB
- Recherche vectorielle optimisÃ©e

### Limites
- DÃ©pendant de l'API OpenAI (latence rÃ©seau)
- CoÃ»t des embeddings et du LLM
- Taille maximale des chunks (contexte LLM)

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature
3. Commit les changements
4. Push vers la branche
5. CrÃ©er une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT.